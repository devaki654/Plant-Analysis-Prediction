{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617ea55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of healthy leaf images in training set: 100\n",
      "Number of powder leaf images in training set: 100\n",
      "Number of rusty leaf images in training set: 100\n",
      "Number of banana spot leaf images in training set: 100\n",
      "Number of cotton spot leaf images in training set: 100\n",
      "Number of eggplant spot leaf images in training set: 100\n",
      "Number of potato spot leaf images in training set: 100\n",
      "Number of sugarcane spot leaf images in training set: 100\n",
      "Number of tomato spot leaf images in training set: 100\n",
      "========================================================\n",
      "Number of healthy leaf images in test set: 100\n",
      "Number of powder leaf images in test set: 100\n",
      "Number of rusty leaf images in test set: 100\n",
      "Number of banana spot leaf images in test set: 100\n",
      "Number of cotton spot leaf images in test set: 100\n",
      "Number of eggplant spot leaf images in test set: 100\n",
      "Number of potato spot leaf images in test set: 100\n",
      "Number of sugarcane spot leaf images in test set: 100\n",
      "Number of tomato spot leaf images in test set: 100\n",
      "========================================================\n",
      "Number of healthy leaf images in validation set: 100\n",
      "Number of powder leaf images in validation set: 100\n",
      "Number of rusty leaf images in validation set: 100\n",
      "Number of banana spot leaf images in validation set: 100\n",
      "Number of cotton spot leaf images in validation set: 100\n",
      "Number of eggplant spot leaf images in validation set: 100\n",
      "Number of potato spot leaf images in validation set: 100\n",
      "Number of sugarcane spot leaf images in validation set: 100\n",
      "Number of tomato spot leaf images in validation set: 100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def total_files(folder_path):\n",
    "    # Count the number of files in the folder\n",
    "    num_files = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "    return num_files\n",
    "\n",
    "# Training files\n",
    "# Training dataset paths\n",
    "train_files_healthy = r\"c:\\users\\cuted\\desktop\\train\\healthy\"\n",
    "train_files_powdery = r\"c:\\users\\cuted\\desktop\\train\\powdery\"\n",
    "train_files_rust = r\"c:\\users\\cuted\\desktop\\train\\rust\"\n",
    "train_files_bananaspot = r\"c:\\users\\cuted\\desktop\\train\\bananaspot\"\n",
    "train_files_cottonspot = r\"c:\\users\\cuted\\desktop\\train\\cottonspot\"\n",
    "train_files_eggplantspot = r\"c:\\users\\cuted\\desktop\\train\\eggplantspot\"\n",
    "train_files_potatospot = r\"c:\\users\\cuted\\desktop\\train\\potatospot\"\n",
    "train_files_sugarcanespot = r\"c:\\users\\cuted\\desktop\\train\\sugarcanespot\"  # Corrected path\n",
    "train_files_tomatospot = r\"c:\\users\\cuted\\desktop\\train\\tomatospot\"\n",
    "\n",
    "\n",
    "# Test files\n",
    "test_files_healthy = r\"c:\\users\\cuted\\desktop\\test\\healthy\"\n",
    "test_files_powdery = r\"c:\\users\\cuted\\desktop\\test\\powdery\"\n",
    "test_files_rust = r\"c:\\users\\cuted\\desktop\\test\\rust\"\n",
    "test_files_bananaspot = r\"c:\\users\\cuted\\desktop\\test\\bananaspot\"  # Corrected path\n",
    "test_files_cottonspot = r\"c:\\users\\cuted\\desktop\\test\\cottonspot\"\n",
    "test_files_eggplantspot = r\"c:\\users\\cuted\\desktop\\test\\eggplantspot\"\n",
    "test_files_potatospot = r\"c:\\users\\cuted\\desktop\\test\\potatospot\"\n",
    "test_files_sugarcanespot = r\"c:\\users\\cuted\\desktop\\test\\sugarcanespot\"\n",
    "test_files_tomatospot = r\"c:\\users\\cuted\\desktop\\test\\tomatospot\"\n",
    "\n",
    "# Validation files\n",
    "# Validation dataset paths\n",
    "valid_files_healthy = r\"c:\\users\\cuted\\desktop\\val\\healthy\"\n",
    "valid_files_powdery = r\"c:\\users\\cuted\\desktop\\val\\powdery\"\n",
    "valid_files_rust = r\"c:\\users\\cuted\\desktop\\val\\rust\"\n",
    "valid_files_bananaspot = r\"c:\\users\\cuted\\desktop\\val\\bananaspot\"\n",
    "valid_files_cottonspot = r\"c:\\users\\cuted\\desktop\\val\\cottonspot\"\n",
    "valid_files_eggplantspot = r\"c:\\users\\cuted\\desktop\\val\\eggplantspot\"\n",
    "valid_files_potatospot = r\"c:\\users\\cuted\\desktop\\val\\potatospot\"\n",
    "valid_files_sugarcanespot = r\"c:\\users\\cuted\\desktop\\val\\sugarcanespot\"\n",
    "valid_files_tomatospot = r\"c:\\users\\cuted\\desktop\\val\\tomatospot\"\n",
    "\n",
    "\n",
    "# Print the number of files in each set\n",
    "\n",
    "# Training set\n",
    "print(\"Number of healthy leaf images in training set:\", total_files(train_files_healthy))\n",
    "print(\"Number of powder leaf images in training set:\", total_files(train_files_powdery))\n",
    "print(\"Number of rusty leaf images in training set:\", total_files(train_files_rust))\n",
    "print(\"Number of banana spot leaf images in training set:\", total_files(train_files_bananaspot))\n",
    "print(\"Number of cotton spot leaf images in training set:\", total_files(train_files_cottonspot))\n",
    "print(\"Number of eggplant spot leaf images in training set:\", total_files(train_files_eggplantspot))\n",
    "print(\"Number of potato spot leaf images in training set:\", total_files(train_files_potatospot))\n",
    "print(\"Number of sugarcane spot leaf images in training set:\", total_files(train_files_sugarcanespot))\n",
    "print(\"Number of tomato spot leaf images in training set:\", total_files(train_files_tomatospot))\n",
    "\n",
    "print(\"========================================================\")\n",
    "\n",
    "# Test set\n",
    "print(\"Number of healthy leaf images in test set:\", total_files(test_files_healthy))\n",
    "print(\"Number of powder leaf images in test set:\", total_files(test_files_powdery))\n",
    "print(\"Number of rusty leaf images in test set:\", total_files(test_files_rust))\n",
    "print(\"Number of banana spot leaf images in test set:\", total_files(test_files_bananaspot))\n",
    "print(\"Number of cotton spot leaf images in test set:\", total_files(test_files_cottonspot))\n",
    "print(\"Number of eggplant spot leaf images in test set:\", total_files(test_files_eggplantspot))\n",
    "print(\"Number of potato spot leaf images in test set:\", total_files(test_files_potatospot))\n",
    "print(\"Number of sugarcane spot leaf images in test set:\", total_files(test_files_sugarcanespot))\n",
    "print(\"Number of tomato spot leaf images in test set:\", total_files(test_files_tomatospot))\n",
    "\n",
    "print(\"========================================================\")\n",
    "\n",
    "# Validation set\n",
    "print(\"Number of healthy leaf images in validation set:\", total_files(valid_files_healthy))\n",
    "print(\"Number of powder leaf images in validation set:\", total_files(valid_files_powdery))\n",
    "print(\"Number of rusty leaf images in validation set:\", total_files(valid_files_rust))\n",
    "print(\"Number of banana spot leaf images in validation set:\", total_files(valid_files_bananaspot))\n",
    "print(\"Number of cotton spot leaf images in validation set:\", total_files(valid_files_cottonspot))\n",
    "print(\"Number of eggplant spot leaf images in validation set:\", total_files(valid_files_eggplantspot))\n",
    "print(\"Number of potato spot leaf images in validation set:\", total_files(valid_files_potatospot))\n",
    "print(\"Number of sugarcane spot leaf images in validation set:\", total_files(valid_files_sugarcanespot))\n",
    "print(\"Number of tomato spot leaf images in validation set:\", total_files(valid_files_tomatospot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b358374a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\users\\cuted\\desktop\\train\\healthy: 300 images\n",
      "c:\\users\\cuted\\desktop\\train\\powdery: 300 images\n",
      "c:\\users\\cuted\\desktop\\train\\rust: 300 images\n",
      "c:\\users\\cuted\\desktop\\train\\bananaspot: 300 images\n",
      "c:\\users\\cuted\\desktop\\train\\cottonspot: 300 images\n",
      "c:\\users\\cuted\\desktop\\train\\eggplantspot: 300 images\n",
      "c:\\users\\cuted\\desktop\\train\\potatospot: 300 images\n",
      "c:\\users\\cuted\\desktop\\train\\sugarcanespot: 300 images\n",
      "c:\\users\\cuted\\desktop\\train\\tomatospot: 300 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_images(folder_path):\n",
    "    return len(os.listdir(folder_path))\n",
    "\n",
    "# Paths to all class folders\n",
    "folders = [\n",
    "    r\"c:\\users\\cuted\\desktop\\train\\healthy\",\n",
    "    r\"c:\\users\\cuted\\desktop\\train\\powdery\",\n",
    "    r\"c:\\users\\cuted\\desktop\\train\\rust\",\n",
    "    r\"c:\\users\\cuted\\desktop\\train\\bananaspot\",\n",
    "    r\"c:\\users\\cuted\\desktop\\train\\cottonspot\",\n",
    "    r\"c:\\users\\cuted\\desktop\\train\\eggplantspot\",\n",
    "    r\"c:\\users\\cuted\\desktop\\train\\potatospot\",\n",
    "    r\"c:\\users\\cuted\\desktop\\train\\sugarcanespot\",\n",
    "    r\"c:\\users\\cuted\\desktop\\train\\tomatospot\",\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    print(f\"{folder}: {count_images(folder)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7de1a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 4.3660, Accuracy: 48.61%\n",
      "Validation Loss: 0.9803, Validation Accuracy: 69.06%\n",
      "Epoch 2/15, Loss: 0.8283, Accuracy: 71.94%\n",
      "Validation Loss: 0.6030, Validation Accuracy: 79.28%\n",
      "Epoch 3/15, Loss: 0.6638, Accuracy: 76.78%\n",
      "Validation Loss: 0.4161, Validation Accuracy: 84.67%\n",
      "Epoch 4/15, Loss: 0.4937, Accuracy: 82.94%\n",
      "Validation Loss: 0.3991, Validation Accuracy: 85.39%\n",
      "Epoch 5/15, Loss: 0.4668, Accuracy: 83.56%\n",
      "Validation Loss: 0.3112, Validation Accuracy: 90.33%\n",
      "Epoch 6/15, Loss: 0.4087, Accuracy: 85.06%\n",
      "Validation Loss: 0.2610, Validation Accuracy: 91.72%\n",
      "Epoch 7/15, Loss: 0.3318, Accuracy: 88.67%\n",
      "Validation Loss: 0.2566, Validation Accuracy: 90.83%\n",
      "Epoch 8/15, Loss: 0.3339, Accuracy: 87.72%\n",
      "Validation Loss: 0.2592, Validation Accuracy: 91.28%\n",
      "Epoch 9/15, Loss: 0.4154, Accuracy: 85.39%\n",
      "Validation Loss: 0.1895, Validation Accuracy: 92.83%\n",
      "Epoch 10/15, Loss: 0.2771, Accuracy: 90.33%\n",
      "Validation Loss: 0.2486, Validation Accuracy: 90.44%\n",
      "Epoch 11/15, Loss: 0.1991, Accuracy: 92.83%\n",
      "Validation Loss: 0.1110, Validation Accuracy: 97.00%\n",
      "Epoch 12/15, Loss: 0.1471, Accuracy: 95.61%\n",
      "Validation Loss: 0.1063, Validation Accuracy: 96.94%\n",
      "Epoch 13/15, Loss: 0.1465, Accuracy: 95.39%\n",
      "Validation Loss: 0.1020, Validation Accuracy: 97.11%\n",
      "Epoch 14/15, Loss: 0.1286, Accuracy: 95.78%\n",
      "Validation Loss: 0.0939, Validation Accuracy: 97.33%\n",
      "Epoch 15/15, Loss: 0.1092, Accuracy: 96.61%\n",
      "Validation Loss: 0.0877, Validation Accuracy: 97.61%\n",
      "Test Accuracy: 97.56%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "\n",
    "# Check for device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the improved CNN architecture\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)  # Number of classes\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(-1, 128 * 16 * 16)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model and move to device\n",
    "model = CNNModel().to(device)\n",
    "\n",
    "# Data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=r\"C:\\Users\\cuted\\Desktop\\train\", transform=transform)\n",
    "valid_dataset = datasets.ImageFolder(root=r\"C:\\Users\\cuted\\Desktop\\val\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=r\"C:\\Users\\cuted\\Desktop\\test\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Initialize loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, epochs=15):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                valid_total += labels.size(0)\n",
    "                valid_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        valid_accuracy = 100 * valid_correct / valid_total\n",
    "        print(f\"Validation Loss: {valid_loss/len(valid_loader):.4f}, Validation Accuracy: {valid_accuracy:.2f}%\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler)\n",
    "\n",
    "# Test the model\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Test the model\n",
    "test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c44a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 5.9841, Accuracy: 40.67%\n",
      "Validation Loss: 1.6292, Validation Accuracy: 52.78%\n",
      "Epoch 2/30, Loss: 1.1473, Accuracy: 66.56%\n",
      "Validation Loss: 0.6131, Validation Accuracy: 75.78%\n",
      "Epoch 3/30, Loss: 0.7002, Accuracy: 74.00%\n",
      "Validation Loss: 0.5851, Validation Accuracy: 78.67%\n",
      "Epoch 4/30, Loss: 0.5304, Accuracy: 80.56%\n",
      "Validation Loss: 0.3574, Validation Accuracy: 87.33%\n",
      "Epoch 5/30, Loss: 0.4861, Accuracy: 81.11%\n",
      "Validation Loss: 0.3205, Validation Accuracy: 89.00%\n",
      "Epoch 6/30, Loss: 0.4800, Accuracy: 82.44%\n",
      "Validation Loss: 0.3193, Validation Accuracy: 87.78%\n",
      "Epoch 7/30, Loss: 0.5196, Accuracy: 82.89%\n",
      "Validation Loss: 0.4720, Validation Accuracy: 84.78%\n",
      "Epoch 8/30, Loss: 0.4665, Accuracy: 83.44%\n",
      "Validation Loss: 0.2361, Validation Accuracy: 91.11%\n",
      "Epoch 9/30, Loss: 0.3406, Accuracy: 87.67%\n",
      "Validation Loss: 0.2165, Validation Accuracy: 91.44%\n",
      "Epoch 10/30, Loss: 0.3105, Accuracy: 88.33%\n",
      "Validation Loss: 0.1521, Validation Accuracy: 95.00%\n",
      "Epoch 11/30, Loss: 0.2142, Accuracy: 94.44%\n",
      "Validation Loss: 0.1664, Validation Accuracy: 95.00%\n",
      "Epoch 12/30, Loss: 0.1483, Accuracy: 95.22%\n",
      "Validation Loss: 0.1345, Validation Accuracy: 96.67%\n",
      "Epoch 13/30, Loss: 0.1596, Accuracy: 94.67%\n",
      "Validation Loss: 0.1131, Validation Accuracy: 96.67%\n",
      "Epoch 14/30, Loss: 0.1902, Accuracy: 95.89%\n",
      "Validation Loss: 0.1146, Validation Accuracy: 97.11%\n",
      "Epoch 15/30, Loss: 0.1483, Accuracy: 95.78%\n",
      "Validation Loss: 0.1126, Validation Accuracy: 97.56%\n",
      "Epoch 16/30, Loss: 0.1661, Accuracy: 96.11%\n",
      "Validation Loss: 0.1031, Validation Accuracy: 97.67%\n",
      "Epoch 17/30, Loss: 0.1660, Accuracy: 96.67%\n",
      "Validation Loss: 0.1036, Validation Accuracy: 97.56%\n",
      "Epoch 18/30, Loss: 0.1283, Accuracy: 97.22%\n",
      "Validation Loss: 0.0982, Validation Accuracy: 98.11%\n",
      "Epoch 19/30, Loss: 0.1229, Accuracy: 96.56%\n",
      "Validation Loss: 0.0956, Validation Accuracy: 97.78%\n",
      "Epoch 20/30, Loss: 0.1438, Accuracy: 96.89%\n",
      "Validation Loss: 0.0959, Validation Accuracy: 98.00%\n",
      "Epoch 21/30, Loss: 0.1079, Accuracy: 97.33%\n",
      "Validation Loss: 0.0868, Validation Accuracy: 98.56%\n",
      "Epoch 22/30, Loss: 0.1080, Accuracy: 97.33%\n",
      "Validation Loss: 0.0880, Validation Accuracy: 98.22%\n",
      "Epoch 23/30, Loss: 0.1352, Accuracy: 97.22%\n",
      "Validation Loss: 0.0906, Validation Accuracy: 98.11%\n",
      "Epoch 24/30, Loss: 0.1315, Accuracy: 96.78%\n",
      "Validation Loss: 0.0942, Validation Accuracy: 97.89%\n",
      "Epoch 25/30, Loss: 0.1007, Accuracy: 97.78%\n",
      "Validation Loss: 0.0817, Validation Accuracy: 98.67%\n",
      "Epoch 26/30, Loss: 0.1085, Accuracy: 97.44%\n",
      "Validation Loss: 0.0814, Validation Accuracy: 98.44%\n",
      "Epoch 27/30, Loss: 0.1191, Accuracy: 96.67%\n",
      "Validation Loss: 0.0761, Validation Accuracy: 98.33%\n",
      "Epoch 28/30, Loss: 0.1888, Accuracy: 97.44%\n",
      "Validation Loss: 0.0936, Validation Accuracy: 98.00%\n",
      "Epoch 29/30, Loss: 0.1137, Accuracy: 97.44%\n",
      "Validation Loss: 0.0795, Validation Accuracy: 98.56%\n",
      "Epoch 30/30, Loss: 0.1080, Accuracy: 96.67%\n",
      "Validation Loss: 0.0800, Validation Accuracy: 98.33%\n",
      "Test Accuracy: 98.11%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "\n",
    "# Check for device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the CNN architecture\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 9)  # Adjust for the number of classes\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(-1, 128 * 16 * 16)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model and move to device\n",
    "model = CNNModel().to(device)\n",
    "\n",
    "# Data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=r\"C:\\Users\\cuted\\Desktop\\train\", transform=transform)\n",
    "valid_dataset = datasets.ImageFolder(root=r\"C:\\Users\\cuted\\Desktop\\val\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=r\"C:\\Users\\cuted\\Desktop\\test\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Initialize loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, epochs=30):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                valid_total += labels.size(0)\n",
    "                valid_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        valid_accuracy = 100 * valid_correct / valid_total\n",
    "        print(f\"Validation Loss: {valid_loss/len(valid_loader):.4f}, Validation Accuracy: {valid_accuracy:.2f}%\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler)\n",
    "\n",
    "# Test the model\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "      \n",
    "# Test the model\n",
    "test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dcc2bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to c:\\Users\\cuted\\Desktop\\Plant-Disease-Prediction-main\\modelmsql.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Function to save the model and display the save path\n",
    "def save_model(model, save_path=\"modelmsql.pth\"):\n",
    "    try:           \n",
    "        # Get the absolute path of the save path\n",
    "        abs_save_path = os.path.abspath(save_path)\n",
    "        \n",
    "        # Save the model      \n",
    "        torch.save(model.state_dict(), abs_save_path)\n",
    "        \n",
    "        # Print the absolute save path\n",
    "        print(f\"Model saved to {abs_save_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving the model: {e}\")\n",
    "\n",
    "save_model(model, \"modelmsql.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88745317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cuted\\AppData\\Local\\Temp\\ipykernel_3200\\4189901371.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cnn_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 84\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     model \u001b[38;5;241m=\u001b[39m CNNModel()\n\u001b[1;32m---> 84\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcnn_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# Path to the image for prediction\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcuted\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcottonspot\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbact374.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your image path\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 37\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model, model_path)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(model, model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelmsql.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 37\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m         model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m     39\u001b[0m         model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cuted\\anaconda3\\envs\\myconda\\Lib\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\cuted\\anaconda3\\envs\\myconda\\Lib\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\cuted\\anaconda3\\envs\\myconda\\Lib\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cnn_model.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define your CNN model (update this to match the saved model architecture)\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)  # Match the saved model\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 256)  # Match input size to your image dimensions\n",
    "        self.fc2 = nn.Linear(256, 10)  # Match output size to your number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, model_path=\"modelmsql.pth\"):\n",
    "    try:\n",
    "        state_dict = torch.load(model_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        print(\"Ensure the model architecture matches the saved state.\")\n",
    "    return model\n",
    "\n",
    "# Function to make a prediction on an image\n",
    "def predict_image(model, image_path, transform=None):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Ensure the image has 3 channels (RGB)\n",
    "    \n",
    "    # Apply the transformations to the image\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    \n",
    "    # Add a batch dimension (model expects a batch of images)\n",
    "    image = image.unsqueeze(0)\n",
    "    \n",
    "    # Make the prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "    \n",
    "    # Get the predicted class (index of the max value)\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "    \n",
    "    return predicted_class.item()\n",
    "\n",
    "# List of class labels (make sure to update this with your actual class labels)\n",
    "class_labels = [\n",
    "    'Class0', 'Class1', 'Class2', 'Class3', 'Class4', 'Class5', \n",
    "    'Class6', 'Class7', 'Class8', 'Class9'\n",
    "]\n",
    "\n",
    "# Set up the image transformation (ensure it matches the training transformations)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Adjust size to match your training input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Standard normalization\n",
    "])\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the model\n",
    "    model = CNNModel()\n",
    "    model = load_model(model, \"cnn_model.pth\")\n",
    "\n",
    "    # Path to the image for prediction\n",
    "    image_path = r\"C:\\Users\\cuted\\Desktop\\test\\cottonspot\\bact374.jpg\"  # Replace with your image path\n",
    "\n",
    "    # Make a prediction\n",
    "    try:\n",
    "        predicted_class = predict_image(model, image_path, transform)\n",
    "        predicted_class_label = class_labels[predicted_class]\n",
    "        print(f\"Predicted class index: {predicted_class}\")\n",
    "        print(f\"Predicted class label: {predicted_class_label}\")\n",
    "        \n",
    "        # Display all class labels in order\n",
    "        print(\"\\nAll class labels:\")\n",
    "        for i, label in enumerate(class_labels):\n",
    "            print(f\"{i}: {label}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03630a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cuted\\AppData\\Local\\Temp\\ipykernel_3200\\4189901371.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: Error(s) in loading state_dict for CNNModel:\n",
      "\tUnexpected key(s) in state_dict: \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"bn2.num_batches_tracked\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"bn3.num_batches_tracked\". \n",
      "\tsize mismatch for fc1.weight: copying a param with shape torch.Size([512, 32768]) from checkpoint, the shape in current model is torch.Size([256, 32768]).\n",
      "\tsize mismatch for fc1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n",
      "\tsize mismatch for fc2.weight: copying a param with shape torch.Size([9, 512]) from checkpoint, the shape in current model is torch.Size([10, 256]).\n",
      "\tsize mismatch for fc2.bias: copying a param with shape torch.Size([9]) from checkpoint, the shape in current model is torch.Size([10]).\n",
      "Ensure the model architecture matches the saved state.\n",
      "Predicted class index: 1\n",
      "Predicted class label: healthy\n",
      "\n",
      "All class labels:\n",
      "0: eggplantspot\n",
      "1: healthy\n",
      "2: potatospot\n",
      "3: powdery\n",
      "4: rust\n",
      "5: tomatospot\n",
      "6: bananaspot\n",
      "7: chilispot\n",
      "8: cottonspot\n",
      "9: sugarcanespot\n"
     ]
    }
   ],
   "source": [
    "# List of class labels (updated with plant disease categories)\n",
    "class_labels = [\n",
    "    'eggplantspot', 'healthy', 'potatospot', 'powdery', 'rust',\n",
    "    'tomatospot', 'bananaspot', 'chilispot', 'cottonspot', 'sugarcanespot'\n",
    "]\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the model\n",
    "    model = CNNModel()\n",
    "    model = load_model(model, \"modelmsql.pth\")\n",
    "\n",
    "    # Path to the image for prediction\n",
    "    image_path = r\"C:\\Users\\cuted\\Desktop\\test\\cottonspot\\bact374.jpg\"  # Replace with your image path\n",
    "\n",
    "    # Make a prediction\n",
    "    try:\n",
    "        predicted_class = predict_image(model, image_path, transform)\n",
    "        predicted_class_label = class_labels[predicted_class]  # Use the correct class label\n",
    "        print(f\"Predicted class index: {predicted_class}\")\n",
    "        print(f\"Predicted class label: {predicted_class_label}\")\n",
    "        \n",
    "        # Display all class labels in order\n",
    "        print(\"\\nAll class labels:\")\n",
    "        for i, label in enumerate(class_labels):\n",
    "            print(f\"{i}: {label}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a5aedc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully to model.pth\n",
      "Class labels saved successfully to class_labels.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, model_path=\"model.pth\"):\n",
    "    try:\n",
    "        torch.save(model.state_dict(), model_path)  # Save only the model's state_dict\n",
    "        print(f\"Model saved successfully to {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "\n",
    "# Function to save the class labels\n",
    "def save_class_labels(class_labels, labels_path=\"class_labels.pth\"):\n",
    "    try:\n",
    "        torch.save(class_labels, labels_path)  # Save the class labels\n",
    "        print(f\"Class labels saved successfully to {labels_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving class labels: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming model is already defined and trained\n",
    "    model = CNNModel()\n",
    "    \n",
    "    # Save the model and class labels\n",
    "    save_model(model, \"model.pth\")\n",
    "    \n",
    "    # List of class labels (updated with plant disease categories)\n",
    "    class_labels = [\n",
    "            'bananaspot','cottonspot','eggplantspot','healthy','potatospot','powdery','rust','sugarcanespot','tomatospot'\n",
    "        ]\n",
    "    # Save class labels\n",
    "    save_class_labels(class_labels, \"class_labels.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5650b769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cuted\\AppData\\Local\\Temp\\ipykernel_10044\\2269930492.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_data = torch.load(model_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in state_dict:\n",
      "conv1.weight\n",
      "conv1.bias\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "bn1.num_batches_tracked\n",
      "conv2.weight\n",
      "conv2.bias\n",
      "bn2.weight\n",
      "bn2.bias\n",
      "bn2.running_mean\n",
      "bn2.running_var\n",
      "bn2.num_batches_tracked\n",
      "conv3.weight\n",
      "conv3.bias\n",
      "bn3.weight\n",
      "bn3.bias\n",
      "bn3.running_mean\n",
      "bn3.running_var\n",
      "bn3.num_batches_tracked\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "fc2.weight\n",
      "fc2.bias\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to the .pth file\n",
    "model_path = r\"C:\\Users\\cuted\\Desktop\\Plant-Disease-Prediction-main\\model.pth\"\n",
    "\n",
    "# Load the .pth file\n",
    "model_data = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Check if it contains a `state_dict`\n",
    "if isinstance(model_data, dict) and \"state_dict\" in model_data:\n",
    "    state_dict = model_data[\"state_dict\"]\n",
    "elif isinstance(model_data, dict):\n",
    "    state_dict = model_data\n",
    "else:\n",
    "    raise ValueError(\"The .pth file does not contain a state_dict or recognizable structure.\")\n",
    "\n",
    "# Print the keys in the state_dict\n",
    "print(\"Keys in state_dict:\")\n",
    "for key in state_dict.keys():\n",
    "    print(key)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
